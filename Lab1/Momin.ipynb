{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab One\n",
    "### Authors: Eric Smith, Tyler Giallanza, Oscar Vallner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is DeepDream?\n",
    "\n",
    "DeepDream is a technique used for image generation from pre-trained convolutional neural networks. Although the technique is most famous for producing outlandish-looking artistic pictures, there are practical use-cases as well.\n",
    "\n",
    "## Why DeepDream?\n",
    "\n",
    "DeepDream can be used practically to gain insight into how a convolutonal neural network makes classification decisions. By choosing a layer and modifying an input image to maximize the activation outputs of that layer's filters, the final image contains features that represent in some way what the layer activations are most sensitive to.\n",
    "This is very useful for determining what function each layer serves in a trained network. For example, if an image generated by a layer contains a great deal of spirals or swirling shapes, it can be intuited that this layer is sensitive to curvature. For deeper layers, this technique becomes even more useful. Because deeper layers operate at higher levels of abstraction, DeepDream output from a deeper layer often contains these structured objects such as dogs or cats.\n",
    "\n",
    "![img](https://d19fbfhz0hcvd2.cloudfront.net/UC/wp-content/uploads/2015/07/4ZSWD4L-e1436336191130.jpg)\n",
    "Figure 1: DeepDream genererating animals as well as animal body parts (such as eyes).\n",
    "\n",
    "In a network designed for a large-scale classification task, such as discriminating between the 1000 classes that comprise the ImageNet dataset, visualizing higher layers can help yield insight into what the model is most sensitive to predicting. For example, DeepDream visualizations of InceptionV3 often result in images of animals. This information is useful to someone choosing a model for a task that does not involve animals - the high sensitivity of InceptionV3 to animals as demonstrated by DeepDream may indicate that it is not the best model for this particular use-case.\n",
    "\n",
    "## Chosing a Model for DeepDream\n",
    "\n",
    "DeepDream modifies an image to maximally excite layers of a pre-trained model. The choice of model thus directly impacts the quality and structure of the output images. We chose to use VGGNet as our model for DeepDream visualization. We made this choice for a number of reasons.\n",
    "\n",
    "![vgg](https://www.researchgate.net/profile/Kasthurirangan_Gopalakrishnan/publication/319952138/figure/fig2/AS:613973590282251@1523394119133/A-schematic-of-the-VGG-16-Deep-Convolutional-Neural-Network-DCNN-architecture-trained.png)\n",
    "Figure 2: VGG-16 architecture.\n",
    "\n",
    "First, VGG-16 has a very straightforward architecture - all convolutional layers proceed sequentially, unlike the split-transform-merge branched pathway strategy found in Inception-based architectures. This is an appealing feature for a model visualized with DeepDream because it becomes clearer what each layer is responsible for. If we were to use Inception, for example, it would be unclear which layers comprising each block are responsible for the block's visualizations.\n",
    "\n",
    "\n",
    "![inception](https://cdn-images-1.medium.com/max/1600/1*acUVChT9lBW4vKaAKQhOOw.png)\n",
    "Figure 3: An example of a block taken from an Inception-based architecture. DeepDream must operate on the \"filter concatenation\" layer, making it difficult to determine how each layer in the Inception block contributes to the final visualization.\n",
    "\n",
    "Other models of comparable performance on ImageNet that have similar structures, most notably ResNet, are often much deeper, wherein the shallowest version of ResNet still contains 34 layers. Having more layers present in a network yields more options for which layers to include in a DeepDream model. This makes it difficult to search combinations of layers, as the number of such combinations is exponentially larger.\n",
    "\n",
    "Second, our empirical testing of VGGNet and InceptionV3 revealed that VGGNet produced better looking images. We speculate that this is due to the layers of abstraction being more clearly defined between layers. Since each Inception block contains multiple convolutional layers in parallel, whereas VGG layers flow sequentially, the information distillation pipeline is more straightforward for VGGNet. Furthermore, we only tested a few hyperparameter combinations for each architecture rather than exhaustively search the hhyperparameter space. Thus, it is possible that for different parameters Inception would yield better results, but for the parameters we tested VGGNet seemed to be the most logical choice of model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepDream Implementation\n",
    "\n",
    "In this section we will implement DeepDream using Keras. We will start off with an implementation of InceptionV3 that has been pretrained on the ImageNet dataset. This code is heavily based on Francois Chollet's implementation provided in the Deep Learning with Python book and implemented [here](https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/8.2-deep-dream.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.applications import inception_v3\n",
    "from keras.applications import vgg16\n",
    "from keras import backend as K\n",
    "\n",
    "#We are using a pretrained model and thus should disable learning functions\n",
    "K.set_learning_phase(0)\n",
    "\n",
    "#Load the pre-trained VGG16 model based on the imagenet dataset\n",
    "model = vgg16.VGG16(weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['block1_conv1', 'block1_conv2', 'block1_pool', 'block2_conv1', 'block2_conv2', 'block2_pool', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block3_pool', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block4_pool', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'block5_pool']\n"
     ]
    }
   ],
   "source": [
    "print([layer.name for layer in model.layers if layer.name.startswith('block')]) #The names of each layer in the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use gradient ascent to iteratively update the input image. Unlike the example notebook from class that updated the input image to maximize the activation of a single filter, here we update the input image to maximize the activations of all filters in a given layer. Furthermore, we include multiple layers that are weighted differently.\n",
    "\n",
    "Earlier model layers detect features at a lower level of abstraction, and deeper layers detect higher levels of abstraction. For example, the first few layers likely detect edges and textures, whereas later layers likely detect class-relevant information, such as the presence of ears in a dog. Because we want varied levels of abstraction in our output image, we include layers from earlier in the network as well as layers from later in the network.\n",
    "\n",
    "The loss for our gradient ascent is comprised of a weighted sum for the losses of each layer we are examining. Each layer's loss is the L2 norm - the average sum of squares for each filter activation in the layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This defines the amount that each chosen layer contributes to the visualization\n",
    "#Note that layer names are pre-determined by the already created VGG16 model\n",
    "layer_contributions = {\n",
    "    'block2_conv2': 0.2,\n",
    "    'block3_conv3': 6.,\n",
    "    'block4_conv3': 10.,\n",
    "    'block5_conv3': 4.,\n",
    "}\n",
    "\n",
    "#Map each layer object to the name of the layer - this allows us to access the layers we defined in the\n",
    "#layer contributions dictionary\n",
    "layer_dict = dict([(layer.name, layer) for layer in model.layers])\n",
    "\n",
    "#Define the loss function\n",
    "loss = K.variable(0.)\n",
    "for layer_name in layer_contributions:\n",
    "    coeff = layer_contributions[layer_name]\n",
    "    activation = layer_dict[layer_name].output\n",
    "    scaling = K.prod(K.cast(K.shape(activation), 'float32'))\n",
    "    #The loss for each layer is the L2 norm\n",
    "    loss = loss + coeff * K.sum(K.square(activation[:, 2: -2, 2: -2, :])) / scaling #Skip the boarders to make the edges look better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the loss function defined, we can now code the gradient ascent algorithm. Notice that we normalize the gradients at each step with the L1 norm. This step is a \"trick\" that helps training because the gradients un-normalized can occupy a great range of values. Normalization ensures that the gradient occupies a similar range for every iteration. The choice of L1 for normalization as opposed to a different normalization scheme is somewhat arbitrary; this choice worked empirically in the original DeepDream code so we choose to adopt it here as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Layer containing the image we are iteratively updating\n",
    "dream = model.input\n",
    "\n",
    "#Gradients for the image w.r.t. the loss function\n",
    "grads = K.gradients(loss, dream)[0]\n",
    "\n",
    "#L1 gradient normalization trick - add epsilon value to prevent zero division\n",
    "grads /= K.maximum(K.mean(K.abs(grads)), 1e-7)\n",
    "\n",
    "\n",
    "#Return the loss and the gradients by inputing the generated image to the model\n",
    "outputs = [loss, grads]\n",
    "fetch_loss_and_grads = K.function([dream], outputs)\n",
    "\n",
    "#Call the function that gets the model loss and gradients\n",
    "def eval_loss_and_grads(x):\n",
    "    outs = fetch_loss_and_grads([x])\n",
    "    loss_value = outs[0]\n",
    "    grad_values = outs[1]\n",
    "    return loss_value, grad_values\n",
    "\n",
    "#Run the gradient ascent for a given number of iterations and a given step size (learning rate)\n",
    "def gradient_ascent(x, iterations, step, max_loss=None):\n",
    "    for i in range(iterations):\n",
    "        loss_value, grad_values = eval_loss_and_grads(x)\n",
    "        if max_loss is not None and loss_value > max_loss: #stop the iteration if the maximum loss has been reached\n",
    "            break\n",
    "        print('...Loss value at', i, ':', loss_value)\n",
    "        x += step * grad_values\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from keras.preprocessing import image\n",
    "\n",
    "#Resizes a given image by zooming\n",
    "def resize_img(img, size):\n",
    "    img = np.copy(img)\n",
    "    factors = (1,\n",
    "               float(size[0]) / img.shape[1],\n",
    "               float(size[1]) / img.shape[2],\n",
    "               1)\n",
    "    return scipy.ndimage.zoom(img, factors, order=1)\n",
    "\n",
    "#Saves an image to disk\n",
    "def save_img(img, fname):\n",
    "    pil_img = deprocess_image(np.copy(img))\n",
    "    scipy.misc.imsave(fname, pil_img)\n",
    "\n",
    "#Loads images from disk, resizing and formatting them into a format processable by InceptionV3\n",
    "def preprocess_image(image_path):\n",
    "    img = image.load_img(image_path)\n",
    "    img = image.img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = inception_v3.preprocess_input(img)\n",
    "    return img\n",
    "\n",
    "#Converts tensors into an image that can be visualized with matplotlib\n",
    "def deprocess_image(x):\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        x = x.reshape((3, x.shape[2], x.shape[3]))\n",
    "        x = x.transpose((1, 2, 0)) #If the image channels are first flip the order so matplotlib can handle the image\n",
    "    else:\n",
    "        x = x.reshape((x.shape[1], x.shape[2], 3))\n",
    "    x /= 2. \n",
    "    x += 0.5\n",
    "    x *= 255. #Move image into the 0-255 range\n",
    "    x = np.clip(x, 0, 255).astype('uint8') #Clip any pixels that fall outside of the 0-255 range\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To add a source of variation to the images, and to produce \"dream\" artifacts at different scales, we periodically increase the scale of the images. We run a set number of iterations for each scale, for a total of 5 scales. This results in artifacts produced at 5 different sizes, scaling the image up by 40% each time. This trick was also adopted from the original DeepDream paper due to its production of empirically better looking images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'bird.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-7253368e7d65>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m# Format the image so it is interpretable by VGG\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocess_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_image_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m# Determine the different scales for image resizing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-da0da7a74978>\u001b[0m in \u001b[0;36mpreprocess_image\u001b[1;34m(image_path)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;31m#Loads images from disk, resizing and formatting them into a format processable by InceptionV3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpreprocess_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_img\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimg_to_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\momin\\envs\\ml_lab1\\lib\\site-packages\\keras\\preprocessing\\image.py\u001b[0m in \u001b[0;36mload_img\u001b[1;34m(path, grayscale, target_size, interpolation)\u001b[0m\n\u001b[0;32m    360\u001b[0m         raise ImportError('Could not import PIL.Image. '\n\u001b[0;32m    361\u001b[0m                           'The use of `array_to_img` requires PIL.')\n\u001b[1;32m--> 362\u001b[1;33m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpil_image\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    363\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mgrayscale\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'L'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\momin\\envs\\ml_lab1\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(fp, mode)\u001b[0m\n\u001b[0;32m   2541\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2542\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2543\u001b[1;33m         \u001b[0mfp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2544\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2545\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'bird.jpg'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "step = 0.01  # Gradient ascent step size\n",
    "num_octave = 5  # Number of scales at which to run gradient ascent\n",
    "octave_scale = 1.4  # Size ratio between scales\n",
    "iterations = 30  # Number of ascent steps per scale\n",
    "\n",
    "# Don't constrain the loss\n",
    "max_loss = None\n",
    "\n",
    "# Path of the original image fed into DeepDream\n",
    "base_image_path = 'bird.jpg'\n",
    "\n",
    "# Format the image so it is interpretable by VGG\n",
    "img = preprocess_image(base_image_path)\n",
    "\n",
    "# Determine the different scales for image resizing\n",
    "original_shape = img.shape[1:3]\n",
    "successive_shapes = [original_shape]\n",
    "for i in range(1, num_octave):\n",
    "    shape = tuple([int(dim / (octave_scale ** i)) for dim in original_shape])\n",
    "    successive_shapes.append(shape)\n",
    "\n",
    "# Reverse list of shapes, so that they are in increasing order\n",
    "successive_shapes = successive_shapes[::-1]\n",
    "\n",
    "# Resize the Numpy array of the image to our smallest scale\n",
    "original_img = np.copy(img)\n",
    "shrunk_original_img = resize_img(img, successive_shapes[0])\n",
    "\n",
    "#For every shape run gradient ascent for the given number of iterations\n",
    "for shape in successive_shapes:\n",
    "    print('Processing image shape', shape)\n",
    "    img = resize_img(img, shape)\n",
    "    img = gradient_ascent(img,\n",
    "                          iterations=iterations,\n",
    "                          step=step,\n",
    "                          max_loss=max_loss) #Update the image iteratively with gradient ascent\n",
    "    upscaled_shrunk_original_img = resize_img(shrunk_original_img, shape)\n",
    "    same_size_original = resize_img(original_img, shape)\n",
    "    lost_detail = same_size_original - upscaled_shrunk_original_img\n",
    "\n",
    "    img += lost_detail #Trick to add back the missing information when resizing\n",
    "    shrunk_original_img = resize_img(original_img, shape)\n",
    "    save_img(img, fname='bird_at_scale_' + str(shape) + '.png') #For each octave save a copy of the image to disk\n",
    "    plt.show()\n",
    "\n",
    "save_img(img, fname='final_dream.png') #Save the final image to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.imshow(deprocess_image(np.copy(img)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "with open('./Images/animated_bird.gif','rb') as f:\n",
    "    display(Image(data=f.read(), format='png'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
