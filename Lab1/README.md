
# Lab One
### Authors: Eric Smith, Tyler Giallanza, Oscar Vallner, Momin Irfan

## What is DeepDream?

DeepDream is a technique used for image generation from pre-trained convolutional neural networks. Although the technique is most famous for producing outlandish-looking artistic pictures, there are practical use-cases as well.

## Why DeepDream?

DeepDream can be used practically to gain insight into how a convolutonal neural network makes classification decisions. By choosing a layer and modifying an input image to maximize the activation outputs of that layer's filters, the final image contains features that represent in some way what the layer activations are most sensitive to.
This is very useful for determining what function each layer serves in a trained network. For example, if an image generated by a layer contains a great deal of spirals or swirling shapes, it can be intuited that this layer is sensitive to curvature. For deeper layers, this technique becomes even more useful. Because deeper layers operate at higher levels of abstraction, DeepDream output from a deeper layer often contains these structured objects such as dogs or cats.

![img](https://d19fbfhz0hcvd2.cloudfront.net/UC/wp-content/uploads/2015/07/4ZSWD4L-e1436336191130.jpg)
Figure 1: DeepDream genererating animals as well as animal body parts (such as eyes).

In a network designed for a large-scale classification task, such as discriminating between the 1000 classes that comprise the ImageNet dataset, visualizing higher layers can help yield insight into what the model is most sensitive to predicting. For example, DeepDream visualizations of InceptionV3 often result in images of animals. This information is useful to someone choosing a model for a task that does not involve animals - the high sensitivity of InceptionV3 to animals as demonstrated by DeepDream may indicate that it is not the best model for this particular use-case.

## Chosing a Model for DeepDream

DeepDream modifies an image to maximally excite layers of a pre-trained model. The choice of model thus directly impacts the quality and structure of the output images. We chose to use VGGNet as our model for DeepDream visualization. We made this choice for a number of reasons.

![vgg](https://www.researchgate.net/profile/Kasthurirangan_Gopalakrishnan/publication/319952138/figure/fig2/AS:613973590282251@1523394119133/A-schematic-of-the-VGG-16-Deep-Convolutional-Neural-Network-DCNN-architecture-trained.png)
Figure 2: VGG-16 architecture.

First, VGG-16 has a very straightforward architecture - all convolutional layers proceed sequentially, unlike the split-transform-merge branched pathway strategy found in Inception-based architectures. This is an appealing feature for a model visualized with DeepDream because it becomes clearer what each layer is responsible for. If we were to use Inception, for example, it would be unclear which layers comprising each block are responsible for the block's visualizations.


![inception](https://cdn-images-1.medium.com/max/1600/1*acUVChT9lBW4vKaAKQhOOw.png)
Figure 3: An example of a block taken from an Inception-based architecture. DeepDream must operate on the "filter concatenation" layer, making it difficult to determine how each layer in the Inception block contributes to the final visualization.

Other models of comparable performance on ImageNet that have similar structures, most notably ResNet, are often much deeper, wherein the shallowest version of ResNet still contains 34 layers. Having more layers present in a network yields more options for which layers to include in a DeepDream model. This makes it difficult to search combinations of layers, as the number of such combinations is exponentially larger.

Second, our empirical testing of VGGNet and InceptionV3 revealed that VGGNet produced better looking images. We speculate that this is due to the layers of abstraction being more clearly defined between layers. Since each Inception block contains multiple convolutional layers in parallel, whereas VGG layers flow sequentially, the information distillation pipeline is more straightforward for VGGNet. Furthermore, we only tested a few hyperparameter combinations for each architecture rather than exhaustively search the hhyperparameter space. Thus, it is possible that for different parameters Inception would yield better results, but for the parameters we tested VGGNet seemed to be the most logical choice of model.

## DeepDream Implementation

In this section we will implement DeepDream using Keras. We will start off with an implementation of InceptionV3 that has been pretrained on the ImageNet dataset. This code is heavily based on Francois Chollet's implementation provided in the Deep Learning with Python book and implemented [here](https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/8.2-deep-dream.ipynb).


```python
from keras.applications import inception_v3
from keras.applications import vgg16
from keras import backend as K

#We are using a pretrained model and thus should disable learning functions
K.set_learning_phase(0)

#Load the pre-trained VGG16 model based on the imagenet dataset
model = vgg16.VGG16(weights='imagenet', include_top=False)
```

    Using TensorFlow backend.



```python
print([layer.name for layer in model.layers if layer.name.startswith('block')]) #The names of each layer in the model
```

    ['block1_conv1', 'block1_conv2', 'block1_pool', 'block2_conv1', 'block2_conv2', 'block2_pool', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block3_pool', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block4_pool', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'block5_pool']


Next, we use gradient ascent to iteratively update the input image. Unlike the example notebook from class that updated the input image to maximize the activation of a single filter, here we update the input image to maximize the activations of all filters in a given layer. Furthermore, we include multiple layers that are weighted differently.

Earlier model layers detect features at a lower level of abstraction, and deeper layers detect higher levels of abstraction. For example, the first few layers likely detect edges and textures, whereas later layers likely detect class-relevant information, such as the presence of ears in a dog. Because we want varied levels of abstraction in our output image, we include layers from earlier in the network as well as layers from later in the network.

The loss for our gradient ascent is comprised of a weighted sum for the losses of each layer we are examining. Each layer's loss is the L2 norm - the average sum of squares for each filter activation in the layer.


```python
#This defines the amount that each chosen layer contributes to the visualization
#Note that layer names are pre-determined by the already created VGG16 model
layer_contributions = {
    'block2_conv2': 0.02,
    'block3_conv3': 0.03,
    'block4_conv3': .5,
    'block5_conv2': 1.5,
}

#Map each layer object to the name of the layer - this allows us to access the layers we defined in the
#layer contributions dictionary
layer_dict = dict([(layer.name, layer) for layer in model.layers])

#Define the loss function
loss = K.variable(0.)
for layer_name in layer_contributions:
    coeff = layer_contributions[layer_name]
    activation = layer_dict[layer_name].output
    scaling = K.prod(K.cast(K.shape(activation), 'float32'))
    #The loss for each layer is the L2 norm
    loss = loss + coeff * K.sum(K.square(activation[:, 2: -2, 2: -2, :])) / scaling #Skip the boarders to make the edges look better
```

With the loss function defined, we can now code the gradient ascent algorithm. Notice that we normalize the gradients at each step with the L1 norm. This step is a "trick" that helps training because the gradients un-normalized can occupy a great range of values. Normalization ensures that the gradient occupies a similar range for every iteration. The choice of L1 for normalization as opposed to a different normalization scheme is somewhat arbitrary; this choice worked empirically in the original DeepDream code so we choose to adopt it here as well.


```python
#Layer containing the image we are iteratively updating
dream = model.input

#Gradients for the image w.r.t. the loss function
grads = K.gradients(loss, dream)[0]

#L1 gradient normalization trick - add epsilon value to prevent zero division
grads /= K.maximum(K.mean(K.abs(grads)), 1e-7)


#Return the loss and the gradients by inputing the generated image to the model
outputs = [loss, grads]
fetch_loss_and_grads = K.function([dream], outputs)

#Call the function that gets the model loss and gradients
def eval_loss_and_grads(x):
    outs = fetch_loss_and_grads([x])
    loss_value = outs[0]
    grad_values = outs[1]
    return loss_value, grad_values

#Run the gradient ascent for a given number of iterations and a given step size (learning rate)
def gradient_ascent(x, iterations, step, max_loss=None):
    for i in range(iterations):
        loss_value, grad_values = eval_loss_and_grads(x)
        if max_loss is not None and loss_value > max_loss: #stop the iteration if the maximum loss has been reached
            break
        print('...Loss value at', i, ':', loss_value)
        x += step * grad_values
    return x
```


```python
import scipy
from keras.preprocessing import image

#Resizes a given image by zooming
def resize_img(img, size):
    img = np.copy(img)
    factors = (1,
               float(size[0]) / img.shape[1],
               float(size[1]) / img.shape[2],
               1)
    return scipy.ndimage.zoom(img, factors, order=1)

#Saves an image to disk
def save_img(img, fname):
    pil_img = deprocess_image(np.copy(img))
    scipy.misc.imsave(fname, pil_img)

def preprocess_image(image_path):
    # Util function to open, resize and format pictures
    # into appropriate tensors.
    img = image.load_img(image_path)
    img = image.img_to_array(img)
    img = np.expand_dims(img, axis=0)
    img = vgg16.preprocess_input(img)
    
    # These constants are needed to put the VGG 
    # preprocessed inputs on a scale of 0-255
    # These constants were found here: https://asolda.github.io/post/deepdream/ 
    img[:,:,:,0] += 103.939
    img[:,:,:,1] += 116.779
    img[:,:,:,2] += 123.68
    
    # normalize the values to be from -1 to 1
    img /= 255.
    img -= 0.5
    img *= 2
    # Convert from BGR to RGB
    img = img[:,:, :, ::-1]
    return img

#Converts tensors into an image that can be visualized with matplotlib
def deprocess_image(x):
    if K.image_data_format() == 'channels_first':
        x = x.reshape((3, x.shape[2], x.shape[3]))
        x = x.transpose((1, 2, 0)) #If the image channels are first flip the order so matplotlib can handle the image
    else:
        x = x.reshape((x.shape[1], x.shape[2], 3))
    x /= 2. 
    x += 0.5
    x *= 255. #Move image into the 0-255 range
    x = np.clip(x, 0, 255).astype('uint8') #Clip any pixels that fall outside of the 0-255 range
    return x
```

# Impact of different Layers Analysis


Interested in how each layer impacts the final results, we adjusted our Deep Dream process to emphasis each one of the Four layers


```python
# Constants for all Layer Analysis
step = 0.01  # Gradient ascent step size
iterations = 10  # Number of ascent steps per scale
max_loss = None

base_image_path = 'cat.jpg'
```

## Layer 1


```python
import numpy as np
import matplotlib.pyplot as plt

layer_contributions = {
    'block2_conv2': 2.,
}
layer_dict = dict([(layer.name, layer) for layer in model.layers])
loss = K.variable(0.)

for layer_name in layer_contributions:
    coeff = layer_contributions[layer_name]
    activation = layer_dict[layer_name].output
    scaling = K.prod(K.cast(K.shape(activation), 'float32'))
    loss = loss + coeff * K.sum(K.square(activation[:, 2: -2, 2: -2, :])) / scaling
#Call the function that gets the model loss and gradients
def eval_loss_and_grads(x):
    outs = fetch_loss_and_grads([x])
    loss_value = outs[0]
    grad_values = outs[1]
    return loss_value, grad_values

#Run the gradient ascent for a given number of iterations and a given step size (learning rate)
def gradient_ascent(x, iterations, step, max_loss=None):
    for i in range(iterations):
        loss_value, grad_values = eval_loss_and_grads(x)
        if max_loss is not None and loss_value > max_loss: #stop the iteration if the maximum loss has been reached
            break
        print('...Loss value at', i, ':', loss_value)
        x += step * grad_values
    return x

dream = model.input

#Gradients for the image w.r.t. the loss function
grads = K.gradients(loss, dream)[0]

#L1 gradient normalization trick - add epsilon value to prevent zero division
grads /= K.maximum(K.mean(K.abs(grads)), 1e-7)

#Return the loss and the gradients by inputing the generated image to the model
outputs = [loss, grads]
fetch_loss_and_grads = K.function([dream], outputs)


img = preprocess_image(base_image_path)
img = gradient_ascent(img,
                      iterations=iterations,
                      step=step,
                      max_loss=max_loss)

plt.imshow(deprocess_image(np.copy(img)))
plt.show()

save_img(img, fname='layer1_dream.png')
```

    ...Loss value at 0 : 65.6018
    ...Loss value at 1 : 73.3903
    ...Loss value at 2 : 83.0202
    ...Loss value at 3 : 94.0811
    ...Loss value at 4 : 106.229
    ...Loss value at 5 : 119.341
    ...Loss value at 6 : 133.303
    ...Loss value at 7 : 147.976
    ...Loss value at 8 : 163.28
    ...Loss value at 9 : 179.168



![png](readme_images/Lab%20One_final_two_14_1.png)


We see in this image that by allowing layer 1 to be the most dominant we see most exaggerations and perturbations center around edges and shapes. It is true however that there seems to be a lot of background noise in this process as well. This suggests that the earlier layer focuses slightly on shapes but still can be very noisy

## Layer 2


```python
layer_contributions = {
    'block3_conv3': 2.,
}
layer_dict = dict([(layer.name, layer) for layer in model.layers])
loss = K.variable(0.)


for layer_name in layer_contributions:
    coeff = layer_contributions[layer_name]
    activation = layer_dict[layer_name].output
    scaling = K.prod(K.cast(K.shape(activation), 'float32'))
    loss = loss + coeff * K.sum(K.square(activation[:, 2: -2, 2: -2, :])) / scaling
#Call the function that gets the model loss and gradients
def eval_loss_and_grads(x):
    outs = fetch_loss_and_grads([x])
    loss_value = outs[0]
    grad_values = outs[1]
    return loss_value, grad_values

#Run the gradient ascent for a given number of iterations and a given step size (learning rate)
def gradient_ascent(x, iterations, step, max_loss=None):
    for i in range(iterations):
        loss_value, grad_values = eval_loss_and_grads(x)
        if max_loss is not None and loss_value > max_loss: #stop the iteration if the maximum loss has been reached
            break
        print('...Loss value at', i, ':', loss_value)
        x += step * grad_values
    return x

dream = model.input

#Gradients for the image w.r.t. the loss function
grads = K.gradients(loss, dream)[0]

#L1 gradient normalization trick - add epsilon value to prevent zero division
grads /= K.maximum(K.mean(K.abs(grads)), 1e-7)

#Return the loss and the gradients by inputing the generated image to the model
outputs = [loss, grads]
fetch_loss_and_grads = K.function([dream], outputs)


img = preprocess_image(base_image_path)
img = gradient_ascent(img,
                      iterations=iterations,
                      step=step,
                      max_loss=max_loss)

plt.imshow(deprocess_image(np.copy(img)))
plt.show()

save_img(img, fname='layer2_dream.png')
```

    ...Loss value at 0 : 123.095
    ...Loss value at 1 : 161.25
    ...Loss value at 2 : 197.569
    ...Loss value at 3 : 231.935
    ...Loss value at 4 : 264.37
    ...Loss value at 5 : 295.828
    ...Loss value at 6 : 326.992
    ...Loss value at 7 : 357.684
    ...Loss value at 8 : 388.002
    ...Loss value at 9 : 418.235



![png](readme_images/Lab%20One_final_two_17_1.png)


With layer 2 most exaggerated we see a clearer effect of what layer 1 attempted. Shapes and edges are more of the focus in this iteration. There is still background noise and alteration but less so than prior.

## Layer 3


```python
layer_contributions = {
    'block4_conv3': 4.,
}
layer_dict = dict([(layer.name, layer) for layer in model.layers])
loss = K.variable(0.)


for layer_name in layer_contributions:
    coeff = layer_contributions[layer_name]
    activation = layer_dict[layer_name].output
    scaling = K.prod(K.cast(K.shape(activation), 'float32'))
    loss = loss + coeff * K.sum(K.square(activation[:, 2: -2, 2: -2, :])) / scaling
#Call the function that gets the model loss and gradients
def eval_loss_and_grads(x):
    outs = fetch_loss_and_grads([x])
    loss_value = outs[0]
    grad_values = outs[1]
    return loss_value, grad_values

#Run the gradient ascent for a given number of iterations and a given step size (learning rate)
def gradient_ascent(x, iterations, step, max_loss=None):
    for i in range(iterations):
        loss_value, grad_values = eval_loss_and_grads(x)
        if max_loss is not None and loss_value > max_loss: #stop the iteration if the maximum loss has been reached
            break
        print('...Loss value at', i, ':', loss_value)
        x += step * grad_values
    return x

dream = model.input

#Gradients for the image w.r.t. the loss function
grads = K.gradients(loss, dream)[0]

#L1 gradient normalization trick - add epsilon value to prevent zero division
grads /= K.maximum(K.mean(K.abs(grads)), 1e-7)

#Return the loss and the gradients by inputing the generated image to the model
outputs = [loss, grads]
fetch_loss_and_grads = K.function([dream], outputs)


img = preprocess_image(base_image_path)
img = gradient_ascent(img,
                      iterations=iterations,
                      step=step,
                      max_loss=max_loss)

plt.imshow(deprocess_image(np.copy(img)))
plt.show()

save_img(img, fname='layer3_dream.png')
```

    ...Loss value at 0 : 7.37018
    ...Loss value at 1 : 11.5539
    ...Loss value at 2 : 16.5015
    ...Loss value at 3 : 22.3904
    ...Loss value at 4 : 28.5487
    ...Loss value at 5 : 35.0152
    ...Loss value at 6 : 41.6547
    ...Loss value at 7 : 48.2969
    ...Loss value at 8 : 55.245
    ...Loss value at 9 : 62.5096



![png](readme_images/Lab%20One_final_two_20_1.png)


With layer 3 most exaggerated we start to see features beginning to be exaggerated.The ears, neck and face start to pop out. There even appears to be another face emerging near the leg of the cat. This suggests that perhaps the third layer focuses more on faces.

## Layer 4


```python
layer_contributions = {
    'block5_conv2': 4.,
}
layer_dict = dict([(layer.name, layer) for layer in model.layers])
loss = K.variable(0.)


for layer_name in layer_contributions:
    coeff = layer_contributions[layer_name]
    activation = layer_dict[layer_name].output
    scaling = K.prod(K.cast(K.shape(activation), 'float32'))
    loss = loss + coeff * K.sum(K.square(activation[:, 2: -2, 2: -2, :])) / scaling
#Call the function that gets the model loss and gradients
def eval_loss_and_grads(x):
    outs = fetch_loss_and_grads([x])
    loss_value = outs[0]
    grad_values = outs[1]
    return loss_value, grad_values

#Run the gradient ascent for a given number of iterations and a given step size (learning rate)
def gradient_ascent(x, iterations, step, max_loss=None):
    for i in range(iterations):
        loss_value, grad_values = eval_loss_and_grads(x)
        if max_loss is not None and loss_value > max_loss: #stop the iteration if the maximum loss has been reached
            break
        print('...Loss value at', i, ':', loss_value)
        x += step * grad_values
    return x

dream = model.input

#Gradients for the image w.r.t. the loss function
grads = K.gradients(loss, dream)[0]

#L1 gradient normalization trick - add epsilon value to prevent zero division
grads /= K.maximum(K.mean(K.abs(grads)), 1e-7)

#Return the loss and the gradients by inputing the generated image to the model
outputs = [loss, grads]
fetch_loss_and_grads = K.function([dream], outputs)


img = preprocess_image(base_image_path)
img = gradient_ascent(img,
                      iterations=iterations,
                      step=step,
                      max_loss=max_loss)

plt.imshow(deprocess_image(np.copy(img)))
plt.show()

save_img(img, fname='layer4_dream.png')
```

    ...Loss value at 0 : 1.19809
    ...Loss value at 1 : 2.5116
    ...Loss value at 2 : 4.05511
    ...Loss value at 3 : 5.83953
    ...Loss value at 4 : 7.67108
    ...Loss value at 5 : 10.0074
    ...Loss value at 6 : 12.2571
    ...Loss value at 7 : 14.8873
    ...Loss value at 8 : 17.6097
    ...Loss value at 9 : 20.5427



![png](readme_images/Lab%20One_final_two_23_1.png)


With layer 4 most exaggerated the face becomes the focal point of the photo. We see less phantom faces showing up throughout the photo then with layer 3 exaggerated, but the main cat face is the main emphasis. It seems as if layer 4 searches for more well defined faces. 

## Addition of Resizing Noise 

To add a source of variation to the images, and to produce "dream" artifacts at different scales, we periodically increase the scale of the images. We run a set number of iterations for each scale, for a total of 5 scales. This results in artifacts produced at 5 different sizes, scaling the image up by 40% each time. This trick was also adopted from the original DeepDream paper due to its production of empirically better looking images.


```python
### Reset the Loss function to account for all layers

#This defines the amount that each chosen layer contributes to the visualization
#Note that layer names are pre-determined by the already created VGG16 model
layer_contributions = {
    'block2_conv2': 0.02,
    'block3_conv3': 0.03,
    'block4_conv3': .5,
    'block5_conv2': 1.5,
}

#Map each layer object to the name of the layer - this allows us to access the layers we defined in the
#layer contributions dictionary
layer_dict = dict([(layer.name, layer) for layer in model.layers])

#Define the loss function
loss = K.variable(0.)
for layer_name in layer_contributions:
    coeff = layer_contributions[layer_name]
    activation = layer_dict[layer_name].output
    scaling = K.prod(K.cast(K.shape(activation), 'float32'))
    #The loss for each layer is the L2 norm
    loss = loss + coeff * K.sum(K.square(activation[:, 2: -2, 2: -2, :])) / scaling #Skip the boarders to make the edges look better
    
    
#Layer containing the image we are iteratively updating
dream = model.input

#Gradients for the image w.r.t. the loss function
grads = K.gradients(loss, dream)[0]

#L1 gradient normalization trick - add epsilon value to prevent zero division
grads /= K.maximum(K.mean(K.abs(grads)), 1e-7)


#Return the loss and the gradients by inputing the generated image to the model
outputs = [loss, grads]
fetch_loss_and_grads = K.function([dream], outputs)

#Call the function that gets the model loss and gradients
def eval_loss_and_grads(x):
    outs = fetch_loss_and_grads([x])
    loss_value = outs[0]
    grad_values = outs[1]
    return loss_value, grad_values

#Run the gradient ascent for a given number of iterations and a given step size (learning rate)
def gradient_ascent(x, iterations, step, max_loss=None):
    for i in range(iterations):
        loss_value, grad_values = eval_loss_and_grads(x)
        if max_loss is not None and loss_value > max_loss: #stop the iteration if the maximum loss has been reached
            break
        print('...Loss value at', i, ':', loss_value)
        x += step * grad_values
    return x
```


```python
step = 0.01  # Gradient ascent step size
num_octave = 5  # Number of scales at which to run gradient ascent
octave_scale = 1.4  # Size ratio between scales
iterations = 10  # Number of ascent steps per scale

# Don't constrain the loss
max_loss = None

# Format the image so it is interpretable by VGG
img = preprocess_image(base_image_path)

# Determine the different scales for image resizing
original_shape = img.shape[1:3]
successive_shapes = [original_shape]
for i in range(1, num_octave):
    shape = tuple([int(dim / (octave_scale ** i)) for dim in original_shape])
    successive_shapes.append(shape)

# Reverse list of shapes, so that they are in increasing order
successive_shapes = successive_shapes[::-1]

# Resize the Numpy array of the image to our smallest scale
original_img = np.copy(img)
shrunk_original_img = resize_img(img, successive_shapes[0])

#For every shape run gradient ascent for the given number of iterations
for shape in successive_shapes:
    print('Processing image shape', shape)
    img = resize_img(img, shape)
    img = gradient_ascent(img,
                          iterations=iterations,
                          step=step,
                          max_loss=max_loss) #Update the image iteratively with gradient ascent
    upscaled_shrunk_original_img = resize_img(shrunk_original_img, shape)
    same_size_original = resize_img(original_img, shape)
    lost_detail = same_size_original - upscaled_shrunk_original_img

    img += lost_detail #Trick to add back the missing information when resizing
    shrunk_original_img = resize_img(original_img, shape)
    save_img(img, fname='cat_at_scale_' + str(shape) + '.png') #For each octave save a copy of the image to disk
    plt.imshow(deprocess_image(np.copy(img)))
    plt.show()

save_img(img, fname='final_dream.png') #Save the final image to disk
```

    Processing image shape (104, 156)
    ...Loss value at 0 : 5.26657
    ...Loss value at 1 : 6.33609
    ...Loss value at 2 : 7.35218
    ...Loss value at 3 : 8.48659
    ...Loss value at 4 : 9.69084
    ...Loss value at 5 : 10.8749
    ...Loss value at 6 : 12.2634
    ...Loss value at 7 : 13.6029
    ...Loss value at 8 : 15.1096
    ...Loss value at 9 : 16.6964



![png](readme_images/Lab%20One_final_two_28_1.png)


    Processing image shape (145, 218)
    ...Loss value at 0 : 11.8936
    ...Loss value at 1 : 14.6259
    ...Loss value at 2 : 17.148
    ...Loss value at 3 : 19.6634
    ...Loss value at 4 : 22.2285
    ...Loss value at 5 : 25.0061
    ...Loss value at 6 : 27.6161
    ...Loss value at 7 : 30.5192
    ...Loss value at 8 : 33.3118
    ...Loss value at 9 : 36.3326



![png](readme_images/Lab%20One_final_two_28_3.png)


    Processing image shape (204, 306)
    ...Loss value at 0 : 15.7866
    ...Loss value at 1 : 19.765
    ...Loss value at 2 : 23.2573
    ...Loss value at 3 : 26.4962
    ...Loss value at 4 : 29.207
    ...Loss value at 5 : 32.5653
    ...Loss value at 6 : 35.7024
    ...Loss value at 7 : 39.2815
    ...Loss value at 8 : 42.9276
    ...Loss value at 9 : 46.6254



![png](readme_images/Lab%20One_final_two_28_5.png)


    Processing image shape (285, 428)
    ...Loss value at 0 : 20.3547
    ...Loss value at 1 : 24.8833
    ...Loss value at 2 : 28.6843
    ...Loss value at 3 : 32.3663
    ...Loss value at 4 : 36.121
    ...Loss value at 5 : 39.7639
    ...Loss value at 6 : 43.4818
    ...Loss value at 7 : 47.3222
    ...Loss value at 8 : 51.1257
    ...Loss value at 9 : 55.3676



![png](readme_images/Lab%20One_final_two_28_7.png)


    Processing image shape (400, 600)
    ...Loss value at 0 : 22.4759
    ...Loss value at 1 : 26.7528
    ...Loss value at 2 : 30.4703
    ...Loss value at 3 : 34.0108
    ...Loss value at 4 : 37.7385
    ...Loss value at 5 : 41.4617
    ...Loss value at 6 : 45.2166
    ...Loss value at 7 : 49.1128
    ...Loss value at 8 : 53.204
    ...Loss value at 9 : 57.4678



![png](readme_images/Lab%20One_final_two_28_9.png)


Next, we wanted to introduce additional methods to add noise to our photo outside of the scaling of the image above. To do this, we implemented methods to add two additional types of noise: Gaussian and Salt & Pepper.


```python
# This is needed because anaconda was having a hard time finding cv2
import sys
sys.path.append('/Users/oscar/anaconda/envs/myenv/lib/python3.6/site-packages')

import cv2
import random

# adds noise in a normal distribution to every color channel
def add_gaussian_noise(img):
    amount = 0.05

    noise_image1 = np.zeros(img.shape)
    noise_image1 = np.array([cv2.randn(noise_image1[0], (0-amount),amount)])
    
    noise_image2 = np.zeros(img.shape)
    noise_image2 = np.array([cv2.randn(noise_image2[0],(0-amount),amount)])
    noise_image2 = noise_image2[:,:, :, ::-1]
    
    noise_image3 = np.zeros(img.shape)
    noise_image3 = np.array([cv2.randn(noise_image3[0],(0-amount),amount)])
    noise_image3[:,:, :,[0, 1]] = noise_image3[:,:, :,[1, 0]]
    
    img = img + noise_image1 + noise_image2 + noise_image3
    
    return img

def add_sp_noise(img):
    amount_of_noise = 0.002
    noise_percent = 100 * amount_of_noise
    num_p = np.ceil(amount_of_noise * img.shape[1] *img.shape[2] * 0.5)
    for mat in img:
        for i, row in enumerate(mat):
            for j, pixel in enumerate(row):
                r_int = random.randint(0, 10000)
                if r_int <= noise_percent * 100 * 0.5:
                    img[0][i][j][0] = 1
                    img[0][i][j][1] = 1
                    img[0][i][j][2] = 1
                r_int = random.randint(0, 10000)
                if r_int <= noise_percent * 100 * 0.5:
                    img[0][i][j][0] = -1
                    img[0][i][j][1] = -1
                    img[0][i][j][2] = -1

    return img
```

To demonstrate the Salt & Pepper and Gaussian noise, we ran the photo through 5 octaves WITHOUT using gradient ascent to get a deep dream image. This is as a sanity check to verify that the salt & pepper and (perhaps less obviously) the gaussian noise methods are working as intended


```python
step = 0.01  # Gradient ascent step size
num_octave = 5  # Number of scales at which to run gradient ascent
octave_scale = 1.4  # Size ratio between scales
iterations = 10  # Number of ascent steps per scale

# Don't constrain the loss
max_loss = None

# Path of the original image fed into DeepDream
base_image_path = 'cat.jpg'

# Format the image so it is interpretable by VGG
img = preprocess_image(base_image_path)

# Determine the different scales for image resizing
original_shape = img.shape[1:3]
successive_shapes = [original_shape]
for i in range(1, num_octave):
    shape = tuple([int(dim / (octave_scale ** i)) for dim in original_shape])
    successive_shapes.append(shape)

# Reverse list of shapes, so that they are in increasing order
successive_shapes = successive_shapes[::-1]

# Resize the Numpy array of the image to our smallest scale
original_img = np.copy(img)
shrunk_original_img = resize_img(img, successive_shapes[0])

#For every shape run gradient ascent for the given number of iterations
for shape in successive_shapes:
    print('Processing image shape', shape)
    img = add_sp_noise(img)
    img = resize_img(img, shape)
    img = add_gaussian_noise(img)
    upscaled_shrunk_original_img = resize_img(shrunk_original_img, shape)
    same_size_original = resize_img(original_img, shape)
    lost_detail = same_size_original - upscaled_shrunk_original_img

    img += lost_detail #Trick to add back the missing information when resizing
    shrunk_original_img = resize_img(original_img, shape)
    save_img(img, fname='noise_cat_at_scale_' + str(shape) + '.png') #For each octave save a copy of the image to disk
    plt.imshow(deprocess_image(np.copy(img)))
    plt.show()

save_img(img, fname='final_noisy_cat.png') #Save the final image to disk
```

    Processing image shape (104, 156)



![png](readme_images/Lab%20One_final_two_32_1.png)


    Processing image shape (145, 218)



![png](readme_images/Lab%20One_final_two_32_3.png)


    Processing image shape (204, 306)



![png](readme_images/Lab%20One_final_two_32_5.png)


    Processing image shape (285, 428)



![png](readme_images/Lab%20One_final_two_32_7.png)


    Processing image shape (400, 600)



![png](readme_images/Lab%20One_final_two_32_9.png)


As seen by the photos above, the noise methods do in fact seem to be working. Salt & pepper is added at every stage, and the presence of random red, blue, and green colors can be seen.

Next we ran the deep dream process with the additional noise methods added in.


```python
step = 0.01  # Gradient ascent step size
num_octave = 5  # Number of scales at which to run gradient ascent
octave_scale = 1.4  # Size ratio between scales
iterations = 10  # Number of ascent steps per scale

# Don't constrain the loss
max_loss = None

# Path of the original image fed into DeepDream
base_image_path = 'cat.jpg'

# Format the image so it is interpretable by VGG
img = preprocess_image(base_image_path)

# Determine the different scales for image resizing
original_shape = img.shape[1:3]
successive_shapes = [original_shape]
for i in range(1, num_octave):
    shape = tuple([int(dim / (octave_scale ** i)) for dim in original_shape])
    successive_shapes.append(shape)

# Reverse list of shapes, so that they are in increasing order
successive_shapes = successive_shapes[::-1]

# Resize the Numpy array of the image to our smallest scale
original_img = np.copy(img)
shrunk_original_img = resize_img(img, successive_shapes[0])

#For every shape run gradient ascent for the given number of iterations
for shape in successive_shapes:
    print('Processing image shape', shape)
    img = add_sp_noise(img)
    img = resize_img(img, shape)
    img = add_gaussian_noise(img)
    img = gradient_ascent(img,
                          iterations=iterations,
                          step=step,
                          max_loss=max_loss) #Update the image iteratively with gradient ascent
    upscaled_shrunk_original_img = resize_img(shrunk_original_img, shape)
    same_size_original = resize_img(original_img, shape)
    lost_detail = same_size_original - upscaled_shrunk_original_img

    img += lost_detail #Trick to add back the missing information when resizing
    shrunk_original_img = resize_img(original_img, shape)
    save_img(img, fname='extra_noise_cat_at_scale_' + str(shape) + '.png') #For each octave save a copy of the image to disk
    plt.imshow(deprocess_image(np.copy(img)))
    plt.show()

save_img(img, fname='final_noisy_dream.png') #Save the final image to disk
```

    Processing image shape (104, 156)
    ...Loss value at 0 : 5.40222
    ...Loss value at 1 : 6.5338
    ...Loss value at 2 : 7.62041
    ...Loss value at 3 : 8.80852
    ...Loss value at 4 : 10.0228
    ...Loss value at 5 : 11.3511
    ...Loss value at 6 : 12.7802
    ...Loss value at 7 : 14.3661
    ...Loss value at 8 : 16.0712
    ...Loss value at 9 : 17.8939



![png](readme_images/Lab%20One_final_two_35_1.png)


    Processing image shape (145, 218)
    ...Loss value at 0 : 11.996
    ...Loss value at 1 : 15.0774
    ...Loss value at 2 : 17.9266
    ...Loss value at 3 : 20.7829
    ...Loss value at 4 : 23.4585
    ...Loss value at 5 : 26.1987
    ...Loss value at 6 : 29.1617
    ...Loss value at 7 : 32.1234
    ...Loss value at 8 : 35.285
    ...Loss value at 9 : 38.5761



![png](readme_images/Lab%20One_final_two_35_3.png)


    Processing image shape (204, 306)
    ...Loss value at 0 : 16.4411
    ...Loss value at 1 : 20.7231
    ...Loss value at 2 : 24.7187
    ...Loss value at 3 : 28.3217
    ...Loss value at 4 : 31.8775
    ...Loss value at 5 : 35.4608
    ...Loss value at 6 : 39.2931
    ...Loss value at 7 : 43.1584
    ...Loss value at 8 : 47.1205
    ...Loss value at 9 : 51.0707



![png](readme_images/Lab%20One_final_two_35_5.png)


    Processing image shape (285, 428)
    ...Loss value at 0 : 21.0079
    ...Loss value at 1 : 26.017
    ...Loss value at 2 : 30.4154
    ...Loss value at 3 : 34.4878
    ...Loss value at 4 : 38.4903
    ...Loss value at 5 : 42.4089
    ...Loss value at 6 : 46.728
    ...Loss value at 7 : 50.8839
    ...Loss value at 8 : 55.2724
    ...Loss value at 9 : 59.7085



![png](readme_images/Lab%20One_final_two_35_7.png)


    Processing image shape (400, 600)
    ...Loss value at 0 : 22.9625
    ...Loss value at 1 : 27.688
    ...Loss value at 2 : 31.8305
    ...Loss value at 3 : 35.6211
    ...Loss value at 4 : 39.29
    ...Loss value at 5 : 42.9836
    ...Loss value at 6 : 46.7753
    ...Loss value at 7 : 50.7429
    ...Loss value at 8 : 54.867
    ...Loss value at 9 : 58.9636



![png](readme_images/Lab%20One_final_two_35_9.png)


# Final Results

Here we can see the final output of the deep dream of the cat with and without noise.


```python
with open('final_dream.png','rb') as f:
    display(Image(data=f.read(), format='png'))
    
with open('final_noisy_dream.png','rb') as f:
    display(Image(data=f.read(), format='png'))
```


![png](readme_images/Lab%20One_final_two_38_0.png)



![png](readme_images/Lab%20One_final_two_38_1.png)


# Iterative Process Analysis


```python
from IPython.display import display
from IPython.display import Image
with open('cat.gif','rb') as f:
    display(Image(data=f.read(), format='png'))
```


![png](readme_images/Lab%20One_final_two_40_0.png)


We apply the deep dream process to the cat picture above and note the changes as octaves pass. Initially, it seems that the network is wanting to outline the object as it creates noise and emphasis in those areas. This gets clearer to see as we run more octaves. Not only does the cat attract the attention of the network we start to see that the face of the cat begins to become more and more exaggerated. On our final pass it becomes very exaggerated, to the point of adding more below the face of the cat. This hints that perhaps the network works on classification on edges and then to faces. Given that we are using an ImageNet trained network, this seems to be an understandable concusion

## Image with Guassian Filter and Salt & Pepper Noise


```python
with open('noisy_cat.gif','rb') as f:
    display(Image(data=f.read(), format='png'))
```


![png](readme_images/Lab%20One_final_two_43_0.png)


The noisy image has artifacts that pop up after going through the deep dream process. The salt & peper effect adds small noise to the background and the guassian blurs the features slightly. We see that the network takes these perturbations and continues to estimate featues out of them. The noisy picture seems to have multiple faces (near the tail, neck, legs) and eye like objects appear in the background. This could help as a better indicator of what the network tends towards. 
