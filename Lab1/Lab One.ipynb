{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab One\n",
    "### Authors: Eric Smith, Tyler Giallanza, Oscar Vallner"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is DeepDream?\n",
    "\n",
    "DeepDream is a technique used for image generation from pre-trained convolutional neural networks. Although the technique is most famous for producing outlandish-looking artistic pictures, there are practical use-cases as well.\n",
    "\n",
    "## Why DeepDream?\n",
    "\n",
    "DeepDream can be used practically to gain insight into how a convolutonal neural network makes classification decisions. By choosing a layer and modifying an input image to maximize the activation outputs of that layer's filters, the final image contains features that represent in some way what the layer activations are most sensitive to.\n",
    "This is very useful for determining what function each layer serves in a trained network. For example, if an image generated by a layer contains a great deal of spirals or swirling shapes, it can be intuited that this layer is sensitive to curvature. For deeper layers, this technique becomes even more useful. Because deeper layers operate at higher levels of abstraction, DeepDream output from a deeper layer often contains these structured objects such as dogs or cats.\n",
    "\n",
    "![img](https://d19fbfhz0hcvd2.cloudfront.net/UC/wp-content/uploads/2015/07/4ZSWD4L-e1436336191130.jpg)\n",
    "Figure 1: DeepDream genererating animals as well as animal body parts (such as eyes).\n",
    "\n",
    "In a network designed for a large-scale classification task, such as discriminating between the 1000 classes that comprise the ImageNet dataset, visualizing higher layers can help yield insight into what the model is most sensitive to predicting. For example, DeepDream visualizations of InceptionV3 often result in images of animals. This information is useful to someone choosing a model for a task that does not involve animals - the high sensitivity of InceptionV3 to animals as demonstrated by DeepDream may indicate that it is not the best model for this particular use-case.\n",
    "\n",
    "## Chosing a Model for DeepDream\n",
    "\n",
    "DeepDream modifies an image to maximally excite layers of a pre-trained model. The choice of model thus directly impacts the quality and structure of the output images. We chose to use VGGNet as our model for DeepDream visualization. We made this choice for a number of reasons.\n",
    "\n",
    "![vgg](https://www.researchgate.net/profile/Kasthurirangan_Gopalakrishnan/publication/319952138/figure/fig2/AS:613973590282251@1523394119133/A-schematic-of-the-VGG-16-Deep-Convolutional-Neural-Network-DCNN-architecture-trained.png)\n",
    "Figure 2: VGG-16 architecture.\n",
    "\n",
    "First, VGG-16 has a very straightforward architecture - all convolutional layers proceed sequentially, unlike the split-transform-merge branched pathway strategy found in Inception-based architectures. This is an appealing feature for a model visualized with DeepDream because it becomes clearer what each layer is responsible for. If we were to use Inception, for example, it would be unclear which layers comprising each block are responsible for the block's visualizations.\n",
    "\n",
    "\n",
    "![inception](https://cdn-images-1.medium.com/max/1600/1*acUVChT9lBW4vKaAKQhOOw.png)\n",
    "Figure 3: An example of a block taken from an Inception-based architecture. DeepDream must operate on the \"filter concatenation\" layer, making it difficult to determine how each layer in the Inception block contributes to the final visualization.\n",
    "\n",
    "Other models of comparable performance on ImageNet that have similar structures, most notably ResNet, are often much deeper, wherein the shallowest version of ResNet still contains 34 layers. Having more layers present in a network yields more options for which layers to include in a DeepDream model. This makes it difficult to search combinations of layers, as the number of such combinations is exponentially larger.\n",
    "\n",
    "Second, our empirical testing of VGGNet and InceptionV3 revealed that VGGNet produced better looking images. We speculate that this is due to the layers of abstraction being more clearly defined between layers. Since each Inception block contains multiple convolutional layers in parallel, whereas VGG layers flow sequentially, the information distillation pipeline is more straightforward for VGGNet. Furthermore, we only tested a few hyperparameter combinations for each architecture rather than exhaustively search the hhyperparameter space. Thus, it is possible that for different parameters Inception would yield better results, but for the parameters we tested VGGNet seemed to be the most logical choice of model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepDream Implementation\n",
    "\n",
    "In this section we will implement DeepDream using Keras. We will start off with an implementation of InceptionV3 that has been pretrained on the ImageNet dataset. This code is heavily based on Francois Chollet's implementation provided in the Deep Learning with Python book and implemented [here](https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/8.2-deep-dream.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "87916544/87910968 [==============================] - 38s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from keras.applications import inception_v3\n",
    "from keras import backend as K\n",
    "\n",
    "#We are using a pretrained model and thus should disable learning functions\n",
    "K.set_learning_phase(0)\n",
    "\n",
    "#Load the pre-trained inceptionV3 model based on the imagenet dataset\n",
    "model = inception_v3.InceptionV3(weights='imagenet',\n",
    "                                 include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mixed0', 'mixed1', 'mixed2', 'mixed3', 'mixed4', 'mixed5', 'mixed6', 'mixed7', 'mixed8', 'mixed9_0', 'mixed9', 'mixed9_1', 'mixed10']\n"
     ]
    }
   ],
   "source": [
    "print([layer.name for layer in model.layers if layer.name.startswith('mixed')]) #The names of each layer in the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use gradient ascent to iteratively update the input image. Unlike the example notebook from class that updated the input image to maximize the activation of a single filter, here we update the input image to maximize the activations of all filters in a given layer. Furthermore, we include multiple layers that are weighted differently.\n",
    "\n",
    "Earlier model layers detect features at a lower level of abstraction, and deeper layers detect higher levels of abstraction. For example, the first few layers likely detect edges and textures, whereas later layers likely detect class-relevant information, such as the presence of ears in a dog. Because we want varied levels of abstraction in our output image, we include layers from earlier in the network as well as layers from later in the network.\n",
    "\n",
    "The loss for our gradient ascent is comprised of a weighted sum for the losses of each layer we are examining. Each layer's loss is the L2 norm - the average sum of squares for each filter activation in the layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This defines the amount that each chosen layer contributes to the visualization\n",
    "#Note that layer names are pre-determined by the already created InceptionV3 model\n",
    "layer_contributions = {\n",
    "    'mixed2': 0.2,\n",
    "    'mixed3': 3.,\n",
    "    'mixed4': 2.,\n",
    "    'mixed5': 1.5,\n",
    "}\n",
    "\n",
    "#Map each layer object to the name of the layer - this allows us to access the layers we defined in the\n",
    "#layer contributions dictionary\n",
    "layer_dict = dict([(layer.name, layer) for layer in model.layers])\n",
    "\n",
    "#Define the loss function\n",
    "loss = K.variable(0.)\n",
    "for layer_name in layer_contributions:\n",
    "    coeff = layer_contributions[layer_name]\n",
    "    activation = layer_dict[layer_name].output\n",
    "    scaling = K.prod(K.cast(K.shape(activation), 'float32'))\n",
    "    #The loss for each layer is the L2 norm - we ignore the border pixels of each layer to avoid artifacts\n",
    "    #present at the borders\n",
    "    loss = loss + coeff * K.sum(K.square(activation[:, 2: -2, 2: -2, :])) / scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the loss function defined, we can now code the gradient ascent algorithm. Notice that we normalize the gradients at each step with the L1 norm. This step is a \"trick\" that helps training because the gradients un-normalized can occupy a great range of values. Normalization ensures that the gradient occupies a similar range for every iteration. The choice of L1 for normalization is somewhat arbitrary; this choice worked empirically in the original DeepDream code so we choose to adopt it here as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Layer containing the image we are iteratively updating\n",
    "dream = model.input\n",
    "\n",
    "#Gradients for the image w.r.t. the loss function\n",
    "grads = K.gradients(loss, dream)[0]\n",
    "\n",
    "#L1 gradient normalization trick - add epsilon value to prevent zero division\n",
    "grads /= K.maximum(K.mean(K.abs(grads)), 1e-7)\n",
    "\n",
    "\n",
    "#Return the loss and the gradients by inputing the generated image to the model\n",
    "outputs = [loss, grads]\n",
    "fetch_loss_and_grads = K.function([dream], outputs)\n",
    "\n",
    "#Call the function that gets the model loss and gradients\n",
    "def eval_loss_and_grads(x):\n",
    "    outs = fetch_loss_and_grads([x])\n",
    "    loss_value = outs[0]\n",
    "    grad_values = outs[1]\n",
    "    return loss_value, grad_values\n",
    "\n",
    "#Run the gradient ascent for a given number of iterations and a given step size (learning rate)\n",
    "def gradient_ascent(x, iterations, step, max_loss=None):\n",
    "    for i in range(iterations):\n",
    "        loss_value, grad_values = eval_loss_and_grads(x)\n",
    "        if max_loss is not None and loss_value > max_loss: #stop the iteration if the maximum loss has been reached\n",
    "            break\n",
    "        print('...Loss value at', i, ':', loss_value)\n",
    "        x += step * grad_values\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from keras.preprocessing import image\n",
    "\n",
    "#Resizes a given image by zooming\n",
    "def resize_img(img, size):\n",
    "    img = np.copy(img)\n",
    "    factors = (1,\n",
    "               float(size[0]) / img.shape[1],\n",
    "               float(size[1]) / img.shape[2],\n",
    "               1)\n",
    "    return scipy.ndimage.zoom(img, factors, order=1)\n",
    "\n",
    "#Saves an image to disk\n",
    "def save_img(img, fname):\n",
    "    pil_img = deprocess_image(np.copy(img))\n",
    "    scipy.misc.imsave(fname, pil_img)\n",
    "\n",
    "#Loads images from disk, resizing and formatting them into a format processable by InceptionV3\n",
    "def preprocess_image(image_path):\n",
    "    img = image.load_img(image_path)\n",
    "    img = image.img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = inception_v3.preprocess_input(img)\n",
    "    return img\n",
    "\n",
    "#Converts tensors into an image that can be visualized with matplotlib\n",
    "def deprocess_image(x):\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        x = x.reshape((3, x.shape[2], x.shape[3]))\n",
    "        x = x.transpose((1, 2, 0)) #If the image channels are first flip the order so matplotlib can handle the image\n",
    "    else:\n",
    "        x = x.reshape((x.shape[1], x.shape[2], 3))\n",
    "    x /= 2. \n",
    "    x += 0.5\n",
    "    x *= 255. #Move image into the 0-255 range\n",
    "    x = np.clip(x, 0, 255).astype('uint8') #Clip any pixels that fall outside of the 0-255 range\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image shape (344, 516)\n",
      "...Loss value at 0 : 0.360418\n",
      "...Loss value at 1 : 0.505955\n",
      "...Loss value at 2 : 0.627866\n",
      "...Loss value at 3 : 0.662679\n",
      "...Loss value at 4 : 0.803686\n",
      "...Loss value at 5 : 0.942463\n",
      "...Loss value at 6 : 1.04801\n",
      "...Loss value at 7 : 1.17987\n",
      "...Loss value at 8 : 1.06032\n",
      "...Loss value at 9 : 1.36917\n",
      "...Loss value at 10 : 1.50229\n",
      "...Loss value at 11 : 1.74531\n",
      "...Loss value at 12 : 1.68397\n",
      "...Loss value at 13 : 1.9189\n",
      "...Loss value at 14 : 2.37454\n",
      "...Loss value at 15 : 2.14165\n",
      "...Loss value at 16 : 2.6612\n",
      "...Loss value at 17 : 2.99019\n",
      "...Loss value at 18 : 3.09037\n",
      "...Loss value at 19 : 3.32936\n",
      "...Loss value at 20 : 3.70507\n",
      "...Loss value at 21 : 3.67409\n",
      "...Loss value at 22 : 4.49966\n",
      "...Loss value at 23 : 4.71485\n",
      "...Loss value at 24 : 5.49345\n",
      "...Loss value at 25 : 5.45942\n",
      "...Loss value at 26 : 5.61378\n",
      "...Loss value at 27 : 6.31721\n",
      "...Loss value at 28 : 6.70549\n",
      "...Loss value at 29 : 6.82942\n",
      "...Loss value at 30 : 7.37033\n",
      "...Loss value at 31 : 7.97153\n",
      "...Loss value at 32 : 7.79412\n",
      "...Loss value at 33 : 8.9763\n",
      "...Loss value at 34 : 9.51007\n",
      "...Loss value at 35 : 9.49516\n",
      "Processing image shape (481, 722)\n",
      "...Loss value at 0 : 0.872323\n",
      "...Loss value at 1 : 1.69515\n",
      "...Loss value at 2 : 1.87497\n",
      "...Loss value at 3 : 2.20339\n",
      "...Loss value at 4 : 2.5341\n",
      "...Loss value at 5 : 3.10043\n",
      "...Loss value at 6 : 4.13474\n",
      "...Loss value at 7 : 4.82547\n",
      "...Loss value at 8 : 5.62374\n",
      "...Loss value at 9 : 5.97538\n",
      "...Loss value at 10 : 6.32041\n",
      "...Loss value at 11 : 6.57963\n",
      "...Loss value at 12 : 7.34509\n",
      "...Loss value at 13 : 6.92358\n",
      "...Loss value at 14 : 8.86642\n",
      "...Loss value at 15 : 9.64705\n",
      "Processing image shape (674, 1012)\n",
      "...Loss value at 0 : 0.799433\n",
      "...Loss value at 1 : 1.53949\n",
      "...Loss value at 2 : 2.44021\n",
      "...Loss value at 3 : 2.76449\n",
      "...Loss value at 4 : 3.29494\n",
      "...Loss value at 5 : 3.78919\n",
      "...Loss value at 6 : 4.22852\n",
      "...Loss value at 7 : 4.33214\n",
      "...Loss value at 8 : 5.59105\n",
      "...Loss value at 9 : 6.12485\n",
      "...Loss value at 10 : 6.96777\n",
      "...Loss value at 11 : 7.04926\n",
      "...Loss value at 12 : 8.84889\n",
      "...Loss value at 13 : 9.19607\n",
      "Processing image shape (944, 1416)\n",
      "...Loss value at 0 : 0.841698\n",
      "...Loss value at 1 : 1.56898\n",
      "...Loss value at 2 : 2.30667\n",
      "...Loss value at 3 : 2.70307\n",
      "...Loss value at 4 : 3.33702\n",
      "...Loss value at 5 : 3.71665\n",
      "...Loss value at 6 : 4.63644\n",
      "...Loss value at 7 : 4.71833\n",
      "...Loss value at 8 : 5.52161\n",
      "...Loss value at 9 : 5.78808\n",
      "...Loss value at 10 : 6.62594\n",
      "...Loss value at 11 : 7.82089\n",
      "...Loss value at 12 : 8.24695\n",
      "...Loss value at 13 : 8.40351\n",
      "...Loss value at 14 : 9.49015\n",
      "Processing image shape (1322, 1983)\n",
      "...Loss value at 0 : 0.878268\n",
      "...Loss value at 1 : 1.5839\n",
      "...Loss value at 2 : 2.26044\n",
      "...Loss value at 3 : 2.62758\n",
      "...Loss value at 4 : 3.04914\n",
      "...Loss value at 5 : 3.48128\n",
      "...Loss value at 6 : 3.8209\n",
      "...Loss value at 7 : 4.55815\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-2249864d2c3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m                           \u001b[0miterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                           \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m                           max_loss=max_loss)\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0mupscaled_shrunk_original_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresize_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshrunk_original_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0msame_size_original\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresize_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-c52767abf623>\u001b[0m in \u001b[0;36mgradient_ascent\u001b[0;34m(x, iterations, step, max_loss)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgradient_ascent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mloss_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_loss_and_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmax_loss\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mloss_value\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmax_loss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-c52767abf623>\u001b[0m in \u001b[0;36meval_loss_and_grads\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m#Call the function that gets the model loss and gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0meval_loss_and_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_loss_and_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mgrad_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hpc/applications/anaconda/3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2475\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2476\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hpc/applications/anaconda/3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hpc/applications/anaconda/3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hpc/applications/anaconda/3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/hpc/applications/anaconda/3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hpc/applications/anaconda/3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "step = 0.02  # Gradient ascent step size\n",
    "num_octave = 7  # Number of scales at which to run gradient ascent\n",
    "octave_scale = 1.4  # Size ratio between scales\n",
    "iterations = 50  # Number of ascent steps per scale\n",
    "\n",
    "# If our loss gets larger than 10,\n",
    "# we will interrupt the gradient ascent process, to avoid ugly artifacts\n",
    "max_loss = 10.\n",
    "\n",
    "# Path of the original image fed into DeepDream\n",
    "base_image_path = 'ocean.jpg'\n",
    "\n",
    "# Format the image so it is interpretable by InceptionV3\n",
    "img = preprocess_image(base_image_path)\n",
    "\n",
    "# Determine the different scales for image resizing\n",
    "original_shape = img.shape[1:3]\n",
    "successive_shapes = [original_shape]\n",
    "for i in range(1, num_octave):\n",
    "    shape = tuple([int(dim / (octave_scale ** i)) for dim in original_shape])\n",
    "    successive_shapes.append(shape)\n",
    "\n",
    "# Reverse list of shapes, so that they are in increasing order\n",
    "successive_shapes = successive_shapes[::-1]\n",
    "\n",
    "# Resize the Numpy array of the image to our smallest scale\n",
    "original_img = np.copy(img)\n",
    "shrunk_original_img = resize_img(img, successive_shapes[0])\n",
    "\n",
    "#For every shape run gradient ascent for the given number of iterations\n",
    "for shape in successive_shapes:\n",
    "    print('Processing image shape', shape)\n",
    "    img = resize_img(img, shape)\n",
    "    img = gradient_ascent(img,\n",
    "                          iterations=iterations,\n",
    "                          step=step,\n",
    "                          max_loss=max_loss) #Update the image iteratively with gradient ascent\n",
    "    upscaled_shrunk_original_img = resize_img(shrunk_original_img, shape)\n",
    "    same_size_original = resize_img(original_img, shape)\n",
    "    lost_detail = same_size_original - upscaled_shrunk_original_img\n",
    "\n",
    "    img += lost_detail #Trick to add back the missing information when resizing\n",
    "    shrunk_original_img = resize_img(original_img, shape)\n",
    "    save_img(img, fname='dream_at_scale_' + str(shape) + '.png') #For each octave save a copy of the image to disk\n",
    "    plt.show()\n",
    "\n",
    "save_img(img, fname='final_dream.png') #Save the final image to disk"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
