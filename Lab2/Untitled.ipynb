{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    devices = sess.list_devices()\n",
    "    print(devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "with tf.device('/gpu:0'):\n",
    "    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n",
    "    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n",
    "    c = tf.matmul(a, b)\n",
    "with tf.Session() as sess:\n",
    "    print (sess.run(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A local file was found, but it seems to be incomplete or outdated because the auto file hash does not match the original value of 253f8cb515780f3b799900260a226db6 so we will re-download the data.\n",
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "80142336/80134624 [==============================] - 18s 0us/step\n",
      "Found 0 images belonging to 0 classes.\n",
      "Found 0 images belonging to 0 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ovall\\Anaconda3\\envs\\cuda9\\lib\\site-packages\\ipykernel_launcher.py:43: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "from keras.callbacks import Callback\n",
    "from scipy.misc import imresize, imsave\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from model import EncoderDecoder\n",
    "from util import count_num_samples\n",
    "\n",
    "TRAIN_PATH = 'data'\n",
    "TARGET_SIZE = (256, 256)\n",
    "BATCH_SIZE = 4\n",
    "epochs = 2\n",
    "\n",
    "datagen = ImageDataGenerator()\n",
    "gen = datagen.flow_from_directory(TRAIN_PATH, target_size=TARGET_SIZE,\n",
    "                                  batch_size=BATCH_SIZE, class_mode=None)\n",
    "\n",
    "\n",
    "def create_gen(img_dir, target_size, batch_size):\n",
    "    datagen = ImageDataGenerator()\n",
    "    gen = datagen.flow_from_directory(img_dir, target_size=target_size,\n",
    "                                      batch_size=batch_size, class_mode=None)\n",
    "\n",
    "    def tuple_gen():\n",
    "        for img in gen:\n",
    "            if img.shape[0] != batch_size:\n",
    "                continue\n",
    "\n",
    "            # (X, y)\n",
    "            yield (img, img)\n",
    "\n",
    "    return tuple_gen()\n",
    "\n",
    "# This needs to be in scope where model is defined\n",
    "class OutputPreview(Callback):\n",
    "    def __init__(self, model, test_img_path, increment, preview_dir_path):\n",
    "        test_img = image.load_img(test_img_path)\n",
    "        test_img = imresize(test_img, (256, 256, 3))\n",
    "        test_target = image.img_to_array(test_img)\n",
    "        test_target = np.expand_dims(test_target, axis=0)\n",
    "        self.test_img = test_target\n",
    "        self.model = model\n",
    "\n",
    "        self.preview_dir_path = preview_dir_path\n",
    "\n",
    "        self.increment = increment\n",
    "        self.iteration = 0\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        if (self.iteration % self.increment == 0):\n",
    "            output_img = self.model.predict(self.test_img)[0]\n",
    "            fname = '%d.jpg' % self.iteration\n",
    "            out_path = os.path.join(self.preview_dir_path, fname)\n",
    "            imsave(out_path, output_img)\n",
    "\n",
    "        self.iteration += 1\n",
    "\n",
    "\n",
    "gen = create_gen(TRAIN_PATH, TARGET_SIZE, BATCH_SIZE)\n",
    "\n",
    "num_samples = count_num_samples(TRAIN_PATH)\n",
    "steps_per_epoch = num_samples // BATCH_SIZE\n",
    "\n",
    "# target_layer = int(sys.argv[1])\n",
    "\n",
    "for i in range(1,6):\n",
    "    target_layer = int(i)\n",
    "\n",
    "    encoder_decoder = EncoderDecoder(target_layer=target_layer)\n",
    "\n",
    "    callbacks = [OutputPreview(encoder_decoder, './doge-256.jpg', 5000, './preview-%d' % target_layer)]\n",
    "    encoder_decoder.model.fit_generator(gen, steps_per_epoch=steps_per_epoch,\n",
    "            epochs=epochs, callbacks=callbacks, verbose=0)\n",
    "    encoder_decoder.export_decoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading decoders...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ovall\\Anaconda3\\envs\\cuda9\\lib\\site-packages\\keras\\engine\\saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading VGG...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Styling...\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing import image\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "from scipy.misc import imsave\n",
    "\n",
    "from vgg import VGG19, preprocess_input\n",
    "\n",
    "def get_vgg_features(vgg, inputs, target_layer):\n",
    "    output_layers = [\n",
    "            'block1_conv1',\n",
    "            'block2_conv1',\n",
    "            'block3_conv1',\n",
    "            'block4_conv1',\n",
    "            'block5_conv1'\n",
    "    ]\n",
    "\n",
    "    outputs = [layer.output for layer in vgg.layers\n",
    "               if layer.name == output_layers[target_layer-1]]\n",
    "    f = K.function([vgg.input] + [K.learning_phase()], outputs)\n",
    "    return f([inputs, 1.])\n",
    "\n",
    "\n",
    "def wct(content, style, alpha=0.6, eps=1e-5):\n",
    "    '''\n",
    "    https://github.com/eridgd/WCT-TF/blob/master/ops.py\n",
    "\n",
    "       Perform Whiten-Color Transform on feature maps using numpy\n",
    "       See p.4 of the Universal Style Transfer paper for equations:\n",
    "       https://arxiv.org/pdf/1705.08086.pdf\n",
    "    '''\n",
    "    # 1xHxWxC -> CxHxW\n",
    "    content_t = np.transpose(np.squeeze(content), (2, 0, 1))\n",
    "    style_t = np.transpose(np.squeeze(style), (2, 0, 1))\n",
    "\n",
    "    # CxHxW -> CxH*W\n",
    "    content_flat = content_t.reshape(-1, content_t.shape[1]*content_t.shape[2])\n",
    "    style_flat = style_t.reshape(-1, style_t.shape[1]*style_t.shape[2])\n",
    "\n",
    "    # Whitening transform\n",
    "    mc = content_flat.mean(axis=1, keepdims=True)\n",
    "    fc = content_flat - mc\n",
    "    fcfc = np.dot(fc, fc.T) / (content_t.shape[1]*content_t.shape[2] - 1)\n",
    "    Ec, wc, _ = np.linalg.svd(fcfc)\n",
    "    k_c = (wc > 1e-5).sum()\n",
    "    Dc = np.diag((wc[:k_c]+eps)**-0.5)\n",
    "    fc_hat = Ec[:,:k_c].dot(Dc).dot(Ec[:,:k_c].T).dot(fc)\n",
    "\n",
    "    # Coloring transform\n",
    "    ms = style_flat.mean(axis=1, keepdims=True)\n",
    "    fs = style_flat - ms\n",
    "    fsfs = np.dot(fs, fs.T) / (style_t.shape[1]*style_t.shape[2] - 1)\n",
    "    Es, ws, _ = np.linalg.svd(fsfs)\n",
    "    k_s = (ws > 1e-5).sum()\n",
    "    Ds = np.sqrt(np.diag(ws[:k_s]+eps))\n",
    "    fcs_hat = Es[:,:k_s].dot(Ds).dot(Es[:,:k_s].T).dot(fc_hat)\n",
    "    fcs_hat = fcs_hat + ms\n",
    "\n",
    "    # Blend transform features with original features\n",
    "    blended = alpha*fcs_hat + (1 - alpha)*(fc)\n",
    "\n",
    "    # CxH*W -> CxHxW\n",
    "    blended = blended.reshape(content_t.shape)\n",
    "    # CxHxW -> 1xHxWxC\n",
    "    blended = np.expand_dims(np.transpose(blended, (1,2,0)), 0)\n",
    "\n",
    "    return np.float32(blended)\n",
    "\n",
    "\n",
    "\n",
    "# img_c = image.load_img(sys.argv[1])\n",
    "img_c = image.load_img('./doge-256.jpg')\n",
    "\n",
    "img_c = image.img_to_array(img_c)\n",
    "img_c_shape = img_c.shape\n",
    "img_c = np.expand_dims(img_c, axis=0)\n",
    "\n",
    "# img_s = image.load_img(sys.argv[2])\n",
    "img_s = image.load_img('./data/starrynight.jpg')\n",
    "\n",
    "img_s = image.img_to_array(img_s)\n",
    "img_s_shape = img_s.shape\n",
    "img_s = np.expand_dims(img_s, axis=0)\n",
    "\n",
    "assert img_c_shape == img_s_shape, \\\n",
    "    'Content and style image should be the same shape, %s != %s' \\\n",
    "    % (str(img_c_shape), str(img_s_shape))\n",
    "\n",
    "input_shape = img_c_shape\n",
    "\n",
    "print('Loading decoders...')\n",
    "decoders = {}\n",
    "decoders[1] = load_model('./models/decoder_1.h5')  #/decoder_1.h5\n",
    "decoders[2] = load_model('./models/decoder_2.h5')  #/decoder_2.h5\n",
    "decoders[3] = load_model('./models/decoder_3.h5')  #/decoder_3.h5\n",
    "decoders[4] = load_model('./models/decoder_4.h5')  #/decoder_4.h5\n",
    "decoders[5] = load_model('./models/decoder_5.h5')  #/decoder_5.h5\n",
    "\n",
    "print('Loading VGG...')\n",
    "vgg = VGG19(input_shape=input_shape, target_layer=5)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(np.clip(img_c[0] / 255, 0, 1))\n",
    "plt.show()\n",
    "\n",
    "print('Styling...')\n",
    "for i in [3, 1]:\n",
    "# for i in [1]:\n",
    "    feats_c = get_vgg_features(vgg, img_c, i)\n",
    "    feats_s = get_vgg_features(vgg, img_s, i)\n",
    "    feats_cs = wct(feats_c, feats_s)\n",
    "    img_c = decoders[i].predict(feats_cs)\n",
    "    plt.imshow(np.clip(img_c[0] / 255, 0, 1))\n",
    "    plt.show()\n",
    "\n",
    "print('Saving output...')\n",
    "output_img = img_c[0]\n",
    "\n",
    "imsave(sys.argv[3], output_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Cuda9",
   "language": "python",
   "name": "cuda9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
